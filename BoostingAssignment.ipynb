{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is an ensemble approach(meaning it involves several trees) that starts from a weaker decision and keeps on building the \n",
    "models such that the final prediction is the weighted sum of all the weaker decision-makers. The weights are assigned based on \n",
    "the performance of an individual tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tHow do boosting and bagging differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In boosting we build base models sequentially and in bagging we build parallely. In Boosting we "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhat are week and strong classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A week classifer is a decision tree model with single depth that predicts the data. Generally the accuracy is poor. \n",
    "Strong clssfier is the combination of multiple week classifiers in order to increase the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhy are trees deemed fit for boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- computational scalability\n",
    "- handles missing values\n",
    "- robust to outliers\n",
    "- does not require feature scaling\n",
    "- can deal with irrelevant inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tExplain the step by step implementation of ADA Boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In Adaboost , multiple decision tree week learners are build sequentially. The depth of the tree will be one and is called as Stump.\n",
    "The complete data is feeded into the first week learner and based on the prediction if the model has misclassifed data, then that\n",
    "data / records are given higher weightage. The new updated data is then feeded to the next week learner and the process goes\n",
    "on till we dont have any misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat are pseudo residuals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It is the difference between the individual target value and average target values. It is calculated for each records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tExplain the step by step implementation of Gradient boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here we dont add or increment the misclassified data but we try to optimize the loss function. \n",
    "1. Calculate the average of the label column as initially this average shall minimise the total error.\n",
    "2. Calculate the pseudo residuals.\n",
    "3. create a tree to predict the pseudo residuals instead of a tree to predict for the actual column values.\n",
    "4. new result= previous result+learning rate* residual \n",
    "5. Repeat these steps until the residual stops decreasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tExplain the step by step implementation of XGBoost Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost improves the gradient boosting method. It regularises data better than normal gradient boosted Trees.\n",
    "1. Initialise the tree with only one leaf.\n",
    "2. compute the similarity using the formula  Similarity = Gradient**2/hessian+λ\n",
    "3. Now for splitting data into a tree form, calculate Gain=leftsimilarity+rightsimilarity−similarityforroot\n",
    "4. For tree pruning, the parameter γ is used. The algorithm starts from the lowest level of the tree and then \n",
    "starts pruning based on the value of γ\n",
    "5. Learning is done using the equation  NewValue=oldValue+η∗prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat are the advantages of XGBoost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. out of the box feature of appropriate bias-variance trade-off\n",
    "2. great computation speed \n",
    "3. uses hardware optimization\n",
    "4. works well even if the features are correlated\n",
    "5. robust even if there is noise for classification problem\n",
    "6. the facility of early stopping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
